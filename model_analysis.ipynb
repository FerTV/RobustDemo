{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02be784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141cb2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(y_test, y_pred_labels):\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5769d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model analysis function\n",
    "def model_analysis(model_path):\n",
    "    # Load the pre-trained model\n",
    "    model = torch.load(os.path.join(os.getcwd(), model_path))\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    \n",
    "    # Display the model summary\n",
    "    print(model)\n",
    "\n",
    "    # Function to count the model's trainable parameters\n",
    "    def count_params(model):\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    num_params = count_params(model)\n",
    "    print(f'Number of trainable parameters: {num_params}')\n",
    "\n",
    "    # Define image transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),  # Convert images to tensors\n",
    "        transforms.Normalize((0.5,), (0.5,))  # Normalize the data\n",
    "    ])\n",
    "\n",
    "    # Load the MNIST test dataset\n",
    "    test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    # Model evaluation on the test dataset\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    images_sample = []\n",
    "    labels_sample = []\n",
    "    preds_sample = []\n",
    "\n",
    "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)  # Pass the images through the model\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the predictions\n",
    "            total += labels.size(0)  # Count the number of samples\n",
    "            correct += (predicted == labels).sum().item()  # Count correct predictions\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "            # Save some images and predictions for visualization\n",
    "            if len(images_sample) < 10:\n",
    "                images_sample.extend(images[:10].cpu())\n",
    "                labels_sample.extend(labels[:10].cpu().numpy())\n",
    "                preds_sample.extend(predicted[:10].cpu().numpy())\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = correct / total\n",
    "    print(f'Model accuracy on the MNIST test set: {accuracy * 100:.2f}%')\n",
    "\n",
    "    # Accuracy plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "    ax.plot(all_labels[:100], all_preds[:100], 'bo', label='Predictions vs True Labels')\n",
    "    ax.set_xlabel('True Labels')\n",
    "    ax.set_ylabel('Predictions')\n",
    "    ax.set_title('Comparison of Predictions with True Labels')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Image predictions visualization\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
    "    axes = axes.ravel()\n",
    "    for i in np.arange(10):\n",
    "        axes[i].imshow(images_sample[i].squeeze(), cmap='gray')\n",
    "        axes[i].set_title(f\"True: {labels_sample[i]} Pred: {preds_sample[i]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    report(all_labels, all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c08ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# models to analyze\n",
    "model0_path = \"robust/models/18_02_2025_13_26_47/participant_1_round_0_model.pth\"\n",
    "model4_path = \"robust/models/18_02_2025_13_26_47/participant_1_round_4_model.pth\"\n",
    "model9_path = \"robust/models/18_02_2025_13_26_47/participant_1_round_9_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37349fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze first model\n",
    "model_analysis(model0_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c53f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze mid model\n",
    "model_analysis(model4_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5196f1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze last model\n",
    "model_analysis(model9_path)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "robust-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
